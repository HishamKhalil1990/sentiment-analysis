{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0ZCrfhvodag"
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNgwRw5VzGLl"
   },
   "source": [
    "### install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IEIMRqUaojv4",
    "outputId": "78abf39e-28c8-48fd-dce9-9b21f5b8b31f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement as (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for as\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.6)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: demoji in /usr/local/lib/python3.7/dist-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install numpy as np\n",
    "# !pip install sklearn\n",
    "# !pip install matplotlib\n",
    "# !pip install demoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaknSbepzOSC"
   },
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFFKLSNQzR1a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import demoji\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UP1aY0koYXrt"
   },
   "source": [
    "# Uploading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJXJ68oHYeBS"
   },
   "source": [
    "### uploading stemmed tweets, vocabs and tokenization matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8M0BHuj0G-2",
    "outputId": "1e211062-54b5-4ede-e689-c844df9c983d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data 44861 44861\n",
      "test data 11426 11426\n",
      "vocabs 9285 9285\n",
      "all data 56287 tokenization 56287\n"
     ]
    }
   ],
   "source": [
    "data_folder = 'sample_data'\n",
    "data_folder = os.path.join(os.curdir,data_folder)\n",
    "stemmed_tweets_path = os.path.join(data_folder,'stemmed_tweets.json')\n",
    "vocab_path = os.path.join(data_folder,'vocabs.json')\n",
    "tokenization_path = os.path.join(data_folder,'tokenization_matrix.json')\n",
    "stemmed_tweets = {}\n",
    "vocabs = {}\n",
    "tokenization_list = {}\n",
    "with open(stemmed_tweets_path,encoding='utf-8') as f:\n",
    "  stemmed_tweets = json.load(f)\n",
    "\n",
    "with open(vocab_path,encoding='utf-8') as f:\n",
    "  vocabs = json.load(f)\n",
    "\n",
    "with open(tokenization_path, 'r') as f:\n",
    "  tokenization_list = json.load(f)\n",
    "\n",
    "train_input = stemmed_tweets[\"train_input\"]\n",
    "train_output = stemmed_tweets[\"train_output\"]\n",
    "test_input = stemmed_tweets[\"test_input\"]\n",
    "test_output = stemmed_tweets[\"test_output\"]\n",
    "\n",
    "vocab_to_index = vocabs[\"vocab_to_index\"]\n",
    "index_to_vocab = vocabs[\"index_to_vocab\"]\n",
    "\n",
    "tokenization_matrix = tokenization_list['tokenization_matrix']\n",
    "test_data_tokenization_matrix = tokenization_list['test_data_tokenization_matrix']\n",
    "train_data_tokenization_matrix = tokenization_list['train_data_tokenization_matrix']\n",
    "\n",
    "print(\"train data\",len(train_input),len(train_output))\n",
    "print(\"test data\",len(test_input),len(test_output))\n",
    "print(\"vocabs\",len(vocab_to_index),len(index_to_vocab))\n",
    "print(\"all data\",len(test_input) + len(train_input), \"tokenization\", len(tokenization_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkGtEMxzt0xF"
   },
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHvrKrxtt6HW"
   },
   "source": [
    "### replace emojis with texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z24ZWRjbuB01",
    "outputId": "670aefe9-f256-425e-e1ce-f0cf3e3dd065"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets length: for original is 56287 and for emojis as text is 56287\n",
      "vocab to index size: for original is 9285 and for emojis as text is 9285\n"
     ]
    }
   ],
   "source": [
    "# train data first because dataset for tokenization were based in concatenation of (all_stemmed = pd.concat([selected_train_data,selected_test_data])) \n",
    "all_tweets = train_input + test_input\n",
    "all_tweets_text_emojis = []\n",
    "vocab_to_index_text_emojis = {key:value for (key,value) in vocab_to_index.items()}\n",
    "emojis_list = {}\n",
    "for tweet in all_tweets:\n",
    "  emojis = demoji.findall(tweet)\n",
    "  for emoji in emojis.keys():\n",
    "    value = emojis.get(emoji).replace(\" \",\"\").replace(\"-\",\"\")\n",
    "    if value == 'None' or value == None:\n",
    "      print(emoji)\n",
    "    tweet = tweet.replace(emoji,value)\n",
    "    if value not in vocab_to_index_text_emojis:\n",
    "      emoji_index = vocab_to_index_text_emojis.get(emoji)\n",
    "      try:\n",
    "        vocab_to_index_text_emojis.pop(emoji)\n",
    "        vocab_to_index_text_emojis.update({value:emoji_index})\n",
    "        emojis_list.update({emoji:value})\n",
    "      except:\n",
    "        continue\n",
    "  all_tweets_text_emojis.append(tweet)\n",
    "print(f\"tweets length: for original is {len(all_tweets)} and for emojis as text is {len(all_tweets_text_emojis)}\")\n",
    "print(f\"vocab to index size: for original is {len(vocab_to_index)} and for emojis as text is {len(vocab_to_index_text_emojis)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vgcxGmsw4c9"
   },
   "source": [
    "### vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DPu4UI83w3pE",
    "outputId": "50a31495-2395-48ef-fa9e-945a5c0a1fe1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:1323: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  \"Upper case characters found in\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9285 9285\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=1.0,\n",
    "    ngram_range=(1,1),\n",
    "    vocabulary=vocab_to_index_text_emojis\n",
    ")\n",
    "tfidf_list = vectorizer.fit_transform(all_tweets_text_emojis)\n",
    "tfidf_list = tfidf_list.toarray()\n",
    "names = vectorizer.get_feature_names_out()\n",
    "print(len(names),len(vocab_to_index_text_emojis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IcfIS4BjNEG"
   },
   "source": [
    "### vectorization check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKtn2fmsjUuL",
    "outputId": "e5a23740-0240-4a3e-f7d0-0a5b3f81bca0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets that have [OOV] 9273\n",
      "original tweet ŸàŸÇÿπ ÿßÿ∞ÿß ÿ¨ÿßÿ™ ÿØÿßÿ± ÿ®ÿ¥ŸÅ ŸÉŸÖŸÑ ÿ≠ŸäŸÜ ÿßÿ≠ÿ≥ ÿßÿ≠ÿØ ŸÜŸÇÿµ üíî\n",
      "original tweet tokens [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 141, 148, 620, 1563, 1953, 212, 186, 185, 205, 1235, 2]\n",
      "sorted tweet tokens after removing 0,1 and duplictes [2, 141, 148, 185, 186, 205, 212, 620, 1235, 1563, 1953]\n",
      "token corresponded tfidf value: \n",
      "2 0.15448686769909925\n",
      "141 0.268936326101859\n",
      "148 0.26805461209689146\n",
      "185 0.27272003463292516\n",
      "186 0.272198169246356\n",
      "205 0.2744106313223294\n",
      "212 0.2756022476933053\n",
      "620 0.3234840404601987\n",
      "1235 0.3626546200686223\n",
      "1563 0.37680300530625127\n",
      "1953 0.39253933344312447\n",
      "token - tfidf dictionary {2: 0.15448686769909925, 141: 0.268936326101859, 148: 0.26805461209689146, 185: 0.27272003463292516, 186: 0.272198169246356, 205: 0.2744106313223294, 212: 0.2756022476933053, 620: 0.3234840404601987, 1235: 0.3626546200686223, 1563: 0.37680300530625127, 1953: 0.39253933344312447}\n",
      "tweet tokens subsituted with tfidf values [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.268936326101859, 0.26805461209689146, 0.3234840404601987, 0.37680300530625127, 0.39253933344312447, 0.2756022476933053, 0.272198169246356, 0.27272003463292516, 0.2744106313223294, 0.3626546200686223, 0.15448686769909925]\n"
     ]
    }
   ],
   "source": [
    "# number of tweets that have '[OOV]'\n",
    "oov_tweets_index = [index for index,tokenized_tweet in enumerate(tokenization_matrix) if 1 in tokenized_tweet]\n",
    "print(\"number of tweets that have [OOV]\",len(oov_tweets_index))\n",
    "# subsitute tweet token with coressponded tfidf value \n",
    "index = 1\n",
    "start = 0 # 0 when using train dataset and len(train_input) when using test dataset\n",
    "dataset = train_input\n",
    "matrix = train_data_tokenization_matrix\n",
    "print('original tweet',dataset[index])\n",
    "print('original tweet tokens',matrix[index])\n",
    "sorted_tokenized_token = list(set(matrix[index]))\n",
    "sorted_tokenized_token = [int(token) for token in sorted_tokenized_token if token != 0 and token != 1]\n",
    "sorted_tokenized_token.sort(reverse = False)\n",
    "print('sorted tweet tokens after removing 0,1 and duplictes',sorted_tokenized_token)\n",
    "print(\"token corresponded tfidf value: \")\n",
    "row = tfidf_list[start + index]\n",
    "tfidf_dict = {}\n",
    "for pos in range(len(names)):\n",
    "  if names[pos] in all_tweets_text_emojis[start +index].split(\" \"):\n",
    "    tfidf_value = row[pos]\n",
    "    print(vocab_to_index_text_emojis.get(names[pos]),tfidf_value)\n",
    "    tfidf_dict.update({vocab_to_index_text_emojis.get(names[pos]):tfidf_value})\n",
    "print('token - tfidf dictionary',tfidf_dict)\n",
    "if(len(tfidf_dict) == len(sorted_tokenized_token)):\n",
    "  subsituted_tweet_tokens = [tfidf_dict.get(token) if token != 0 and token != 1 else 0 for token in matrix[index]]\n",
    "  print('tweet tokens subsituted with tfidf values',subsituted_tweet_tokens)\n",
    "else:\n",
    "  print('token or tokens with no tfidf value')\n",
    "  print('sorted tweet tokens after removing 0,1 and duplictes',sorted_tokenized_token)\n",
    "  print(\"token corresponded tfidf value: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3g16O35KCAf"
   },
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9H01F78Dipe4"
   },
   "source": [
    "### encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHhkjgLJKBSS"
   },
   "outputs": [],
   "source": [
    "def encode(\n",
    "    dataset:list, \n",
    "    matrix:list, \n",
    "    start:int, \n",
    "    index:int, \n",
    "    encoded_matrix:list, \n",
    "    error_indeces:list, \n",
    "    error_tokens:list, \n",
    "    tfidf_dict:dict\n",
    "    ):\n",
    "  sorted_tokenized_token = list(set(matrix[index]))\n",
    "  sorted_tokenized_token = [int(token) for token in sorted_tokenized_token if token != 0 and token != 1]\n",
    "  sorted_tokenized_token.sort(reverse = False)\n",
    "  row = tfidf_list[start + index]\n",
    "  tfidf_dict = tfidf_dict\n",
    "  for pos in range(len(names)):\n",
    "    if names[pos] in all_tweets_text_emojis[start +index].split(\" \"):\n",
    "      tfidf_value = row[pos]\n",
    "      tfidf_dict.update({vocab_to_index_text_emojis.get(names[pos]):tfidf_value})\n",
    "  if(len(tfidf_dict) == len(sorted_tokenized_token)):\n",
    "    subsituted_tweet_tokens = [tfidf_dict.get(token) if token != 0 and token != 1 else 0 for token in matrix[index]]\n",
    "    encoded_matrix.append(subsituted_tweet_tokens)\n",
    "  else:\n",
    "    error_indeces.append(start + index)\n",
    "    ini_tfidf_dict = {}\n",
    "    for token in sorted_tokenized_token:\n",
    "      if token not in tfidf_dict:\n",
    "        error_tokens.append(token)\n",
    "        ini_tfidf_dict.update({token:0.0})\n",
    "    encode(\n",
    "        dataset=dataset, \n",
    "        matrix=matrix, \n",
    "        start=start, \n",
    "        index=index, \n",
    "        encoded_matrix=encoded_matrix, \n",
    "        error_indeces=error_indeces, \n",
    "        error_tokens=error_tokens,\n",
    "        tfidf_dict=ini_tfidf_dict\n",
    "        )\n",
    "    # print('error in tweet token encoding at index',index)\n",
    "    # print(f'for index {start + index}')\n",
    "    # print(f'for sorted tokenized token {sorted_tokenized_token}')\n",
    "    # print(f'for tfidf dict {tfidf_dict}')\n",
    "    # print()\n",
    "\n",
    "def encode_token(dataset:list, matrix:list, start:int) -> list:\n",
    "  encoded_matrix = []\n",
    "  error_indeces = []\n",
    "  error_tokens = []\n",
    "  for index in range(len(matrix)):\n",
    "    ini_tfidf_dict = {}\n",
    "    encode(\n",
    "        dataset=dataset, \n",
    "        matrix=matrix, start=start, \n",
    "        index=index, \n",
    "        encoded_matrix=encoded_matrix, \n",
    "        error_indeces=error_indeces, \n",
    "        error_tokens=error_tokens,\n",
    "        tfidf_dict=ini_tfidf_dict\n",
    "        )\n",
    "  return encoded_matrix,error_indeces,error_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xb5zcvp8pT8F"
   },
   "outputs": [],
   "source": [
    "train_encoded_tokens_matrix,train_error_indeces,train_error_tokens = encode_token(dataset=train_input, matrix=train_data_tokenization_matrix, start=0)\n",
    "test_encoded_tokens_matrix,test_error_indeces,test_error_tokens = encode_token(dataset=test_input, matrix=test_data_tokenization_matrix, start=len(train_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aSu9fBqnzkaz",
    "outputId": "5b5039ea-40d3-4c62-93b4-559a090f96ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of error in token-encodeing is15\n",
      "indeces for tokens row are [22584, 27795, 33554, 36443, 37475, 37675, 37809, 39766, 40411, 40888, 44491, 53163, 54961, 56114, 56173]\n",
      "no. of token with no tf-idf is 2\n",
      "tokens with no tf-idf are (['‚è∞ÿßŸÑÿ≥ÿ≠ÿ®', '‚Ä¢‚è∞'], [4764, 7975])\n",
      "train tokenized tokens matrix length 44861\n",
      "train encoded tokens matrix length 44861\n",
      "test tokenized tokens matrix length 11426\n",
      "test encoded tokens matrix length 11426\n"
     ]
    }
   ],
   "source": [
    "error_indeces = train_error_indeces + test_error_indeces\n",
    "error_tokens_index = list(set(train_error_tokens + test_error_tokens))\n",
    "error_tokens_vocabs = [index_to_vocab.get(str(token)) for token in error_tokens_index]\n",
    "print(f\"no. of error in token-encodeing is{len(error_indeces)}\")\n",
    "print(f\"indeces for tokens row are {error_indeces}\")\n",
    "print(f'no. of token with no tf-idf is {len(error_tokens_index)}')\n",
    "print(f'tokens with no tf-idf are {error_tokens_vocabs,error_tokens_index}')\n",
    "print(f'train tokenized tokens matrix length {len(train_data_tokenization_matrix)}')\n",
    "print(f'train encoded tokens matrix length {len(train_encoded_tokens_matrix)}')\n",
    "print(f'test tokenized tokens matrix length {len(test_data_tokenization_matrix)}')\n",
    "print(f'test encoded tokens matrix length {len(test_encoded_tokens_matrix)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SS1W7_lYNtmb",
    "outputId": "bcf55d7d-08b4-41a6-94c7-26a7bcd05ebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ex. for train input dataset before and after encodeing\n",
      "encoded tokens [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2008758878190134, 0.18872435011640437, 0.35472829282851, 0.2868684683572954, 0.5469554531880279, 0.5469554531880279, 0.38915912594795826, 0.32135897603536173, 0.2138846529422816, 0.29329436788329316, 0.17395804649362281]\n",
      "encoded tokens length 26\n",
      "real tokens [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 38, 26, 2325, 670, 476, 476, 3906, 1357, 68, 778, 8]\n",
      "real tokens length 26\n",
      "ex. for test input dataset before and after encodeing\n",
      "encoded tokens [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.580742894652676, 0.580742894652676, 0.2869169032177081, 0.4385284873291479, 0, 0.22549243239991712]\n",
      "encoded tokens length 26\n",
      "real tokens [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9156, 8340, 98, 1833, 1, 8]\n",
      "real tokens length26\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print('ex. for train input dataset before and after encodeing')\n",
    "print(f'encoded tokens {train_encoded_tokens_matrix[index]}')\n",
    "print(f'encoded tokens length {len(train_encoded_tokens_matrix[index])}')\n",
    "print(f'real tokens {train_data_tokenization_matrix[index]}')\n",
    "print(f'real tokens length {len(train_data_tokenization_matrix[index])}')\n",
    "print('ex. for test input dataset before and after encodeing')\n",
    "print(f'encoded tokens {test_encoded_tokens_matrix[index]}')\n",
    "print(f'encoded tokens length {len(test_encoded_tokens_matrix[index])}')\n",
    "print(f'real tokens {test_data_tokenization_matrix[index]}')\n",
    "print(f'real tokens length{len(test_data_tokenization_matrix[index])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuMjaUSsT3vi"
   },
   "source": [
    "### save encoding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVOZOfCvT2z2"
   },
   "outputs": [],
   "source": [
    "encoding_list = dict({\n",
    "    'test_data_encoding_matrix':test_encoded_tokens_matrix,\n",
    "    'train_data_encoding_matrix':train_encoded_tokens_matrix\n",
    "})\n",
    "encoding_matrix_path = os.path.join(data_folder,'encoding_matrix.json')\n",
    "\n",
    "with open(encoding_matrix_path, \"wt+\") as f:\n",
    "    json.dump(obj=encoding_list, fp=f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rNgwRw5VzGLl",
    "GaknSbepzOSC",
    "DJXJ68oHYeBS",
    "nHvrKrxtt6HW",
    "5vgcxGmsw4c9",
    "7IcfIS4BjNEG",
    "9H01F78Dipe4",
    "IuMjaUSsT3vi"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
